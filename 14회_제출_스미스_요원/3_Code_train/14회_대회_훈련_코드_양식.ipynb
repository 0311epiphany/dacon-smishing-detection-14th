{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dacon  14회 KB 금융문자 분석 모델링 경진대회\n",
    "## 스미스 요원(팀명)\n",
    "## 2020년 1월 12일 (제출날짜)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델링 코드 작성방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A 코드 관련\n",
    "\n",
    "1) 입상자는 코드 제출 필수. 제출 코드는 예측 결과를 리더보드 점수로 복원할 수 있어야 함\n",
    "\n",
    "2) 코드 제출시 확장자가 R user는 R or .rmd. Python user는 .py or .ipynb\n",
    "\n",
    "3) 코드에 ‘/data’ 데이터 입/출력 경로 포함 제출 or R의 경우 setwd(\" \"), python의 경우 os.chdir을 활용하여 경로 통일\n",
    "\n",
    "4) 전체 프로세스를 일목요연하게 정리하여 주석을 포함하여 하나의 파일로 제출\n",
    "\n",
    "5) 모든 코드는 오류 없이 실행되어야 함(라이브러리 로딩 코드 포함되어야 함).\n",
    "\n",
    "6) 코드와 주석의 인코딩은 모두 UTF-8을 사용하여야 함\n",
    "\n",
    " \n",
    "B 외부 데이터 관련\n",
    "\n",
    "1) 외부 공공 데이터 (날씨 정보 등) 사용이 가능하나, 코드 제출 시 함께 제출\n",
    "\n",
    "2) 공공 데이터 외의 외부 데이터는 법적인 제약이 없는 경우에만 사용 가능\n",
    "\n",
    "3) 외부 데이터를 크롤링할 경우, 크롤링 코드도 함께 제출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 라이브러리 및 데이터\n",
    "## Library & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#필요한 Library 설치\n",
    "!pip install konlpy\n",
    "!pip install wordcloud\n",
    "!pip install selenium\n",
    "!pip install pyperclip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#시드 설정용 라이브러리 import 및 시드 설정\n",
    "import numpy as np\n",
    "np.random.seed(7)\n",
    "import random\n",
    "random.seed(0)\n",
    "import tensorflow as tf\n",
    "if tf.__version__ == '1.15.0' : #1.15.0 버전일 경우 compat.v2 모듈 사용\n",
    "    tf.compat.v2.random.set_seed(8)\n",
    "else : # 1.15.0 버전이 아닐 경우 \n",
    "    tf.random.set_seed(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # 데이터 분석\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "# os.chdir('파일경로')\n",
    "\n",
    "#전처리용 모델 import\n",
    "from konlpy.tag import Komoran\n",
    "from selenium import webdriver\n",
    "import pyperclip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 머신러닝 모델 import\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Dense, SimpleRNN\n",
    "from tensorflow.keras.models import Sequential, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#사용한 데이터 불러오기\n",
    "train = pd.read_csv(\"../0_DATA/train.csv\") # 대회 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 전처리\n",
    "## Data Cleansing & Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = train['text'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "58131it [04:30, 299.70it/s]"
     ]
    }
   ],
   "source": [
    "# train 데이터 전체에 대해 komoran 형태소 분석기 수행\n",
    "\n",
    "new_text = []\n",
    "kom = Komoran(userdic='../user_pos_dic.txt') #사용자 사전 사용시 괄호 안에 userdic='경로/user_pos_dic.txt'\n",
    "for i, document in tqdm(enumerate(text_list)):\n",
    "    pos = kom.pos(document)\n",
    "    clean_words = []\n",
    "    for p in pos:\n",
    "        if p[1] == 'NNP' or p[1] == 'NNG' or p[1] == 'SL':\n",
    "            clean_words.append(p[0])\n",
    "    document = ' '.join(clean_words)\n",
    "    new_text.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#형태소분석 결과를 train 데이터프레임에 추가\n",
    "train['pos'] = pd.Series(new_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawling Code(크롤링 진행 시 기입)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Komoran 형태소 분석기 수행 후, 제대로 형태소분석이 되지 않은 문자들 따로 분류\n",
    "train_non_miss = train[train['pos']!='']\n",
    "train_miss = train[train['pos']=='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#형태소 분석이 되지 않은 문자의 개수\n",
    "len(train_miss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Komoran의 형태소분석이 진행되지 않은 문장들에 대해, 다음 맞춤법 검사기 작동\n",
    "base_url = 'https://alldic.daum.net/grammar_checker.do'\n",
    "#웹드라이버 불러오기\n",
    "driver = webdriver.Chrome('../chromedriver')\n",
    "driver.get(base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "HTTP request 이용 다음 맞춤법 검사기 실행 코드\n",
    "코드 수행 중에 pyperclip 패키지를 통해 OS의 복사/붙여넣기 클립보드를 사용하므로,\n",
    "코드가 돌아가는 중에 다른 곳에서 복사 혹은 붙여넣기를 사용할 경우 데이터에 잘못된 값이 들어갈 수 있으니 주의하시기 바랍니다\n",
    "'''\n",
    "for index in train_miss.index :\n",
    "    text = train_miss.loc[index,'text']\n",
    "    if len(text) <= 1000:\n",
    "        driver.get(base_url)\n",
    "    \n",
    "        #텍스트 보내기\n",
    "        driver.find_element_by_id(\"tfSpelling\").send_keys(text)\n",
    "\n",
    "        #버튼 누르기\n",
    "        driver.find_element_by_id(\"btnCheck\").click()\n",
    "    \n",
    "        try : \n",
    "            #복사 버튼 누르기\n",
    "            driver.find_element_by_id(\"btnCopy\").click()\n",
    "            corrected[index] = pyperclip.paste()\n",
    "        except : #교정할 부분이 없을 경우\n",
    "            corrected[index] = text\n",
    "            pass\n",
    "    else:\n",
    "        text = text[:1000]\n",
    "        driver.get(base_url)\n",
    "    \n",
    "        #텍스트 보내기\n",
    "        driver.find_element_by_id(\"tfSpelling\").send_keys(text)\n",
    "\n",
    "        #버튼 누르기\n",
    "        driver.find_element_by_id(\"btnCheck\").click()\n",
    "    \n",
    "        try : \n",
    "            #복사 버튼 누르기\n",
    "            driver.find_element_by_id(\"btnCopy\").click()\n",
    "            corrected[index] = pyperclip.paste()\n",
    "        except : #교정할 부분이 없을 경우\n",
    "            corrected[index] = text\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_txt=pd.Series(corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#맞춤법 검사가 완료된 후 다시 형태소 분석\n",
    "miss_pos = []\n",
    "for i, document in enumerate(corrected_txt):\n",
    "    pos = kom.pos(document)\n",
    "    clean_words = []\n",
    "    for p in pos:\n",
    "        if p[1] == 'NNP' or p[1] == 'NNG' or p[1] == 'SL':\n",
    "            clean_words.append(p[0])\n",
    "    document = ' '.join(clean_words)\n",
    "    miss_pos.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_miss['pos'] = miss_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#한번 교정후 여전히 komoran이 인식 못하는 데이터 확인\n",
    "train_miss[train_miss['pos']=='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#확인 못하는 행을 제외후 나머지만으로 기존 train_miss를 업데이트\n",
    "train_miss=train_miss[train_miss['pos']!='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_miss와 train_non_miss를 합친 것으로 train을 업데이트\n",
    "train = pd.concat([train_miss, train_non_miss], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#정제된 train 파일 내보내기\n",
    "train.to_csv('train_miss_included_no_userdic.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 탐색적 자료분석\n",
    "## Exploratory Data Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스미싱/비 스미싱 각각에 대해 워드 클라우드 생성\n",
    "# 스미싱/비 스미싱 문자 각각의 길이 히스토그램\n",
    "# 스미싱/비 스미싱 문자 각각의 형태소 분석 후 단어 개수 히스토그램"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 변수 선택 및 모델 구축\n",
    "## Feature Engineering & Initial Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#스미싱, 비스미싱으로 나눈 뒤 트레이닝, 벨리데이션 셋 분리\n",
    "#벨리데이션 셋의 비율은 스미싱/비스미싱 각각 5%\n",
    "train_sm_list=list(train[train['smishing']==1].index)\n",
    "val_train_sm_index=random.sample(train_sm_list, 935)\n",
    "\n",
    "train_nr_list=list(train[train['smishing']!=1].index)\n",
    "val_train_nr_index=random.sample(train_nr_list, 13861)\n",
    "\n",
    "\n",
    "train_sm_index = list(set(train_sm_list).difference(set(val_train_sm_index)))\n",
    "train_nr_index = list(set(train_nr_list).difference(set(val_train_nr_index)))\n",
    "\n",
    "\n",
    "train_sm = train.loc[train_sm_index,:]\n",
    "train_nr = train.loc[train_nr_index,:]\n",
    "training_set = pd.concat([train_sm,train_nr], axis=0)\n",
    "\n",
    "val_train_sm = train.loc[val_train_sm_index,:]\n",
    "val_train_nr = train.loc[val_train_nr_index,:]\n",
    "validation_set = pd.concat([val_train_sm,val_train_nr], axis=0)\n",
    "\n",
    "X_train =training_set['pos'].values.tolist()\n",
    "y_train = training_set['smishing'].values.tolist()\n",
    "\n",
    "X_val =validation_set['pos'].values.tolist()\n",
    "y_val = validation_set['smishing'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#트레이닝 셋과 밸레데이션 셋을 각각 섞어준다. random 시드에 따라 시행 횟수별로 같은 값이 나옴\n",
    "tmp = [[x,y] for x, y in zip(X_train, y_train)]\n",
    "random.shuffle(tmp)\n",
    "\n",
    "tmp1 = [[x,y] for x, y in zip(X_val, y_val)]\n",
    "random.shuffle(tmp1)\n",
    "\n",
    "X_train = [n[0] for n in tmp]\n",
    "y_train = [n[1] for n in tmp]\n",
    "\n",
    "X_val = [n[0] for n in tmp1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_train+X_val\n",
    "y = y_train+y_val\n",
    "\n",
    "#토크나이저로 단어들의 사전을 만들고 각 행마다 단어의 인덱스 시퀀스로 만듦\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "\n",
    "#train 데이터로 학습한 tokenizer를 json파일로 저장\n",
    "word_vocab = tokenizer.word_index\n",
    "with open(\"../wordIndex.json\",\"w\") as json_file :\n",
    "    json_file.write(json.dumps(word_vocab))\n",
    "\n",
    "X_train_1 = tokenizer.texts_to_sequences(X_train)\n",
    "X_val_1 = tokenizer.texts_to_sequences(X_val)\n",
    "\n",
    "\n",
    "max_len=400 # 전체 데이터의 길이는 400으로 맞춘다.\n",
    "X_train_2 = pad_sequences(X_train_1, maxlen=max_len)\n",
    "X_val_2 = pad_sequences(X_val_1, maxlen=max_len) #단어 리스트를 (X_train, 400) 크기의 2D 정수 텐서로 변환\n",
    "\n",
    "word_to_index = tokenizer.word_index\n",
    "vocab_size = len(word_to_index)+1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 모델 학습 및 검증\n",
    "## Model Tuning & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple RNN 모델 \n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 32)) # 임베딩 벡터의 차원은 32\n",
    "model.add(SimpleRNN(32)) # RNN 셀의 hidden_size는 32\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train_2, y_train, epochs=4, batch_size=32, validation_data=(X_val_2, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 저장\n",
    "model.save('../1_Model/np7_random0_tf8_val0.05.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 결과 및 결언\n",
    "## Conclusion & Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력하세요.\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
